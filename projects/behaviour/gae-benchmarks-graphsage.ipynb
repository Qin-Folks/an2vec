{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, link_classification\n",
    "from stellargraph.data import UniformRandomWalk\n",
    "from stellargraph.data import UnsupervisedSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras \n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from stellargraph import globalvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyCall.jlwrap Main.Dataset>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can be simplified once https://github.com/JuliaPy/pyjulia/issues/310 is fixed\n",
    "from julia.api import LibJulia\n",
    "api = LibJulia.load()\n",
    "api.sysimage = os.path.join(base_dir, \"julia/sys.so\")\n",
    "api.init_julia()\n",
    "\n",
    "from julia import Main\n",
    "Main.eval('include(\"' + os.path.join(base_dir, 'julia/dataset.jl') + '\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprop = 0.5\n",
    "bias = False\n",
    "layer_sizes = [32, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load adjacency and features from npz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load npz file\n",
    "cora = np.load(os.path.join(base_dir, 'datasets/gae-benchmarks/cora.npz'))\n",
    "\n",
    "# Extract adjacency and graph\n",
    "adj = sparse.coo_matrix((cora['adjdata'], (cora['adjrow'], cora['adjcol'])))\n",
    "Gnx = nx.from_scipy_sparse_matrix(adj, edge_attribute='label')\n",
    "nx.set_edge_attributes(Gnx, 'cites', 'label')\n",
    "nx.set_node_attributes(Gnx, \"paper\", \"label\")\n",
    "\n",
    "# Extract features\n",
    "node_features = pd.DataFrame(cora['features'], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a nodes test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_julia_edgelist(g):\n",
    "    return 1 + nx.to_pandas_edgelist(Gnx)[['source', 'target']].values\n",
    "\n",
    "def from_julia_edgelist(edgelist):\n",
    "    pd_edgelist = pd.DataFrame(np.array(edgelist) - 1, columns=['source', 'target'])\n",
    "    pd_edgelist['label'] = 'cites'\n",
    "    g = nx.from_pandas_edgelist(pd_edgelist, edge_attr=\"label\")\n",
    "    nx.set_node_attributes(g, \"paper\", \"label\")\n",
    "    return g\n",
    "\n",
    "gtrain_edgelist, nodes_test, nodes_train = Main.Dataset.make_nodes_test_set(to_julia_edgelist(Gnx), testprop)\n",
    "nodes_test = nodes_test - 1\n",
    "nodes_train = nodes_train - 1\n",
    "Gtrain_nx = from_julia_edgelist(gtrain_edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create the Stellargraph with node features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2708, Edges: 5278\n",
      "\n",
      " Node types:\n",
      "  paper: [2708]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [5278]\n",
      "\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 1092, Edges: 1453\n",
      "\n",
      " Node types:\n",
      "  paper: [1092]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [1453]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = sg.StellarGraph(Gnx, node_features=node_features)\n",
    "Gtrain = sg.StellarGraph(Gtrain_nx, node_features=node_features)\n",
    "print(G.info())\n",
    "print(Gtrain.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Specify the other optional parameter values: root nodes, the number of walks to take per node, the length of each walk, and random seed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_nodes_train = list(Gtrain.nodes())\n",
    "assert set(nodes_train).issuperset(actual_nodes_train)\n",
    "number_of_walks = 1\n",
    "length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create the UnsupervisedSampler instance with the relevant parameters passed to it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_samples = UnsupervisedSampler(Gtrain, nodes=actual_nodes_train, length=length, number_of_walks=number_of_walks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph G together with the unsupervised sampler will be used to generate samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Create a node pair generator:**\n",
    "\n",
    "Next, create the node pair generator for sampling and streaming the training data to the model. The node pair generator essentially \"maps\" pairs of nodes `(target, context)` to the input of GraphSAGE: it either takes minibatches of node pairs, or an `UnsupervisedSampler` instance which generates the minibatches of node pairs on demand. The generator samples 2-hop subgraphs with `(target, context)` head nodes extracted from those pairs, and feeds them, together with the corresponding binary labels indicating which pair represent positive or negative sample, to the input layer of the node pair classifier with GraphSAGE node encoder, for SGD updates of the model parameters.\n",
    "\n",
    "Specify:\n",
    "1. The minibatch size (number of node pairs per minibatch).\n",
    "2. The number of epochs for training the model.\n",
    "3. The sizes of 1- and 2-hop neighbor samples for GraphSAGE:\n",
    "\n",
    "Note that the length of `num_samples` list defines the number of layers/iterations in the GraphSAGE encoder. In this example, we are defining a 2-layer GraphSAGE encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 4\n",
    "num_samples = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we show the working of node pair generator with the UnsupervisedSampler, which will generate samples on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GraphSAGELinkGenerator with an estimated 218 batches generated on the fly per epoch.\n"
     ]
    }
   ],
   "source": [
    "train_gen = GraphSAGELinkGenerator(Gtrain, batch_size, num_samples).flow(unsupervised_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model: a 2-layer GraphSAGE encoder acting as node representation learner, with a link classification layer on concatenated `(node1, node2)` node embeddings.\n",
    "\n",
    "GraphSAGE part of the model, with hidden layer sizes of 50 for both GraphSAGE layers, a bias term, and no dropout. (Dropout can be switched on by specifying a positive dropout rate, 0 < dropout < 1).\n",
    "Note that the length of `layer_sizes` list must be equal to the length of `num_samples`, as `len(num_samples)` defines the number of hops (layers) in the GraphSAGE encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(layer_sizes) == len(num_samples)\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=bias, dropout=0.0, normalize=\"l2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sl/.conda/envs/base36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of graphsage, for node pair inputs:\n",
    "x_inp, x_out = graphsage.build(flatten_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final node pair classification layer that takes a pair of nodes' embeddings produced by `graphsage` encoder, applies a binary operator to them to produce the corresponding node pair embedding ('ip' for inner product; other options for the binary operator can be seen by running a cell with `?link_classification` in it), and passes it through a dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method='ip'\n",
    "    )(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack the GraphSAGE encoder and prediction layer into a Keras model, and specify the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sl/.conda/envs/base36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "219/219 [==============================] - 11s 51ms/step - loss: 0.5823 - binary_accuracy: 0.7185\n",
      "Epoch 2/4\n",
      "219/219 [==============================] - 9s 41ms/step - loss: 0.5414 - binary_accuracy: 0.7645\n",
      "Epoch 3/4\n",
      "219/219 [==============================] - 19s 85ms/step - loss: 0.5356 - binary_accuracy: 0.7722\n",
      "Epoch 4/4\n",
      "219/219 [==============================] - 18s 83ms/step - loss: 0.5299 - binary_accuracy: 0.7769\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        use_multiprocessing=False,\n",
    "        workers=0,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings for all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a new node-based model**\n",
    "\n",
    "The `(src, dst)` node pair classifier `model` has two identical node encoders: one for source nodes in the node pairs, the other for destination nodes in the node pairs passed to the model. We can use either of the two identical encoders to evaluate node embeddings. Below we create an embedding model by defining a new Keras model with `x_inp_src` (a list of odd elements in `x_inp`) and `x_out_src` (the 1st element in `x_out`) as input and output, respectively. Note that this model's weights are the same as those of the corresponding node encoder in the previously trained node pair classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp_src = x_inp[0::2]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a node generator to feed graph nodes to `embedding_model`. We want to evaluate node embeddings for all nodes in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = sorted(G.nodes)\n",
    "node_gen = GraphSAGENodeGenerator(G, batch_size, num_samples).flow(node_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `node_gen` to feed all nodes into the embedding model and extract their embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 2s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "emb = embedding_model.predict_generator(node_gen, workers=4, verbose=1)\n",
    "node_embeddings = emb[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the node embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_subject = np.where(cora['labels'])[1][node_ids]\n",
    "#\n",
    "#X = node_embeddings\n",
    "#if X.shape[1] > 2:\n",
    "#    transform = TSNE #PCA \n",
    "#\n",
    "#    trans = transform(n_components=2)\n",
    "#    emb_transformed = pd.DataFrame(trans.fit_transform(X), index=node_ids)\n",
    "#    emb_transformed['label'] = node_subject\n",
    "#else:\n",
    "#    emb_transformed = pd.DataFrame(X, index=node_ids)\n",
    "#    emb_transformed = emb_transformed.rename(columns = {'0': 0, '1': 1})\n",
    "#    emb_transformed['label'] = node_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = 0.7\n",
    "#\n",
    "#fig, ax = plt.subplots(figsize=(7,7))\n",
    "#ax.scatter(emb_transformed[0], emb_transformed[1], c=emb_transformed['label'].astype(\"category\"), \n",
    "#            cmap=\"jet\", alpha=alpha)\n",
    "#ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "#plt.title('{} visualization of GraphSAGE embeddings for cora dataset'.format(transform.__name__))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 50 input features (node embeddings)\n",
    "X = node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.where(cora['labels'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a Logistic Regression classifier on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[nodes_train, :], X[nodes_test, :], y[nodes_train], y[nodes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(verbose=0, solver='liblinear', multi_class=\"ovr\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044132546668579"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739290989660266"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
